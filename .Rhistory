zi = t(Sigma_de%*%matrix(rnorm(p*n), nrow=p, ncol=n)+mu)
z0 = rep(0, n)
xi = exp(cbind(zi,z0))/rowSums(exp(cbind(zi,z0)))
m = rpois(n, lambda=1000)
yi = t(sapply(1:n, function(i)rmultinom(1, size=m[i], prob=xi[i,])))
yi
comp_naive = colMeans(yi/rowSums(yi))
yi_naive_comp_pos = (yi+0.5)/rowSums(yi+0.5)
z_naive = log(yi_naive_comp_pos/yi_naive_comp_pos[,p+1])[,-(p+1)]
mu_0 = colMeans(z_naive)
Sigma_0 = cov(z_naive)
E_q_log_f_y_given_z = function(lambda_i, delta_i, v2_i, i, mi){
tmp1=lgamma(mi)
tmp2=- sum(lgamma(yi[i,yi[i,]!=0]))
tmp3= sum(yi[i,-(p+1)]*lambda_i)
tmp4=-mi*(log(delta_i)-1+1/delta_i+1/delta_i*sum(exp(lambda_i+1/2*v2_i)))
tmp5=tmp1+tmp2+tmp3+tmp4
return(tmp5)
}
E_q_log_f_z =function(lambda_i, v2_i, mu_0, Sigma_0, Sigma_inv, i, mi){
-1/2*log(det(Sigma_0))-p/2*log(2*pi)-1/2*(matrix(lambda_i,1,p)-mu_0)%*%Sigma_inv%*%t(matrix(lambda_i,1,p)-mu_0)-1/2*sum(diag(Sigma_inv)*v2_i)
}
H_q = function(v2_i, i){
sum(1/2*log(2*pi*exp(1)*v2_i))
}
approx_log_f_y = function(lambda_i, v2_i, delta_i, mu_0, Sigma_0, Sigma_inv, i, mi){
E_q_log_f_y_given_z(lambda_i, delta_i, v2_i, i, mi) + E_q_log_f_z(matrix(lambda_i,1,p), v2_i, mu_0, Sigma_0, Sigma_inv, i, mi) + H_q(v2_i, i)
}
## E step: optimize the variational parameters for each cell; numerical optimization
# for cell i:
Sigma_inv = solve(Sigma_0)
update_cell_i = function(yi, mu_0, Sigma_0, Sigma_inv, i, iteration=50, lambda_i=NULL, v2_i=NULL, delta_i=NULL){
if(is.null(lambda_i)){lambda_i  = matrix(rnorm(p*1), 1, p)}else{lambda_i=lambda_i+ matrix(rnorm(p*1), 1, p)*max(abs(lambda_i))/10}
#if(is.null(v2_i)){v2_i = (matrix(rnorm(p*1), 1, p)^2)}else{v2_i = v2_i+matrix(runif(n=p*1, min=0, max=max(v2_i)/10), 1, p)}
v2_i = (matrix(rnorm(p*1), 1, p)^2) # seems difficult for numerical optimization
if(is.null(delta_i)){delta_i=1}else{delta_i = delta_i*runif(1, min=0.8, max=1.2)}
tol = 1e-6
mi = sum(yi[i,])
count=0
# update
while(count<iteration){  # do partial update, to save time
print(count)
count=count+1
# write own v2_i update code; can be separate for each entry
# or can actually solve for root! (rootSolve seems not accurate)
one_v2 = function(v2_i_old_j, j, Sigma_inv, mi, delta_i, lambda_i){
-mi/delta_i*exp(lambda_i[j]+1/2*v2_i_old_j)-1/2*v2_i_old_j*Sigma_inv[j,j]+1/2*log(v2_i_old_j)
}
v2_i_new = sapply(1:p, function(j){print(v2_i[j])
optim(v2_i[j], one_v2, 'Brent',j=j, Sigma_inv = Sigma_inv, mi=mi, delta_i=delta_i, lambda_i=lambda_i, control=list('fnscale'=-1, 'maxit'=iteration))$par}
)
delta_i_new = 1+sum(exp(lambda_i+1/2*v2_i_new))
lambda_i_new = optim(lambda_i, approx_log_f_y, 'CG', v2_i=v2_i_new, delta_i=delta_i_new, mu_0=mu_0, Sigma_0=Sigma_0, Sigma_inv=Sigma_inv,i=i,mi=mi, control=list('fnscale'=-1, 'maxit'=iteration) )$par
delta_i_new = 1+sum(exp(lambda_i_new+1/2*v2_i_new))
# if(abs(delta_i-delta_i_new)<tol & mean(abs(lambda_i - lambda_i_new))< tol & mean(abs(v2_i - v2_i_new))<tol){
#   break
# }
approx_log_f_y_old = approx_log_f_y(lambda_i, v2_i, delta_i, mu_0, Sigma_0, Sigma_inv, i, mi)
approx_log_f_y_new = approx_log_f_y(lambda_i_new, v2_i_new, delta_i_new, mu_0, Sigma_0, Sigma_inv, i, mi)
if(abs((approx_log_f_y_old -approx_log_f_y_new)/approx_log_f_y_old)<tol){
break
}
delta_i = delta_i_new
lambda_i = lambda_i_new
v2_i = v2_i_new
}
return(list(lambda_i=lambda_i, delta_i=delta_i, v2_i=v2_i))
}
## M step: compute the mu and Sigma to maximize the likelihood; closed form
mu_old = mu_0
Sigma_old = Sigma_0
lambda_i_old = matrix(rnorm(p*n), n, p)
v2_i_old = matrix(rnorm(p*n), n, p)^2
delta_i_old = matrix(rnorm(1*n), n, 1)
Sigma_inv_old = solve(Sigma_old)
update_cell_i(yi, mu_0=mu_old, Sigma_0=Sigma_old, Sigma_inv = Sigma_inv_old, i=1, iteration=20)
update_cell_i = function(yi, mu_0, Sigma_0, Sigma_inv, i, iteration=50, lambda_i=NULL, v2_i=NULL, delta_i=NULL){
if(is.null(lambda_i)){lambda_i  = matrix(rnorm(p*1), 1, p)}else{lambda_i=lambda_i+ matrix(rnorm(p*1), 1, p)*max(abs(lambda_i))/10}
#if(is.null(v2_i)){v2_i = (matrix(rnorm(p*1), 1, p)^2)}else{v2_i = v2_i+matrix(runif(n=p*1, min=0, max=max(v2_i)/10), 1, p)}
v2_i = (matrix(rnorm(p*1), 1, p)^2) # seems difficult for numerical optimization
if(is.null(delta_i)){delta_i=1}else{delta_i = delta_i*runif(1, min=0.8, max=1.2)}
tol = 1e-6
mi = sum(yi[i,])
count=0
# update
while(count<iteration){  # do partial update, to save time
print(count)
count=count+1
# write own v2_i update code; can be separate for each entry
# or can actually solve for root! (rootSolve seems not accurate)
one_v2 = function(v2_i_old_j, j, Sigma_inv, mi, delta_i, lambda_i){
-mi/delta_i*exp(lambda_i[j]+1/2*v2_i_old_j)-1/2*v2_i_old_j*Sigma_inv[j,j]+1/2*log(v2_i_old_j)
}
v2_i_new = sapply(1:p, function(j){
optim(v2_i[j], one_v2, 'Brent',j=j, Sigma_inv = Sigma_inv, mi=mi, delta_i=delta_i, lambda_i=lambda_i, control=list('fnscale'=-1, 'maxit'=iteration))$par}
)
delta_i_new = 1+sum(exp(lambda_i+1/2*v2_i_new))
lambda_i_new = optim(lambda_i, approx_log_f_y, 'CG', v2_i=v2_i_new, delta_i=delta_i_new, mu_0=mu_0, Sigma_0=Sigma_0, Sigma_inv=Sigma_inv,i=i,mi=mi, control=list('fnscale'=-1, 'maxit'=iteration) )$par
delta_i_new = 1+sum(exp(lambda_i_new+1/2*v2_i_new))
# if(abs(delta_i-delta_i_new)<tol & mean(abs(lambda_i - lambda_i_new))< tol & mean(abs(v2_i - v2_i_new))<tol){
#   break
# }
approx_log_f_y_old = approx_log_f_y(lambda_i, v2_i, delta_i, mu_0, Sigma_0, Sigma_inv, i, mi)
approx_log_f_y_new = approx_log_f_y(lambda_i_new, v2_i_new, delta_i_new, mu_0, Sigma_0, Sigma_inv, i, mi)
if(abs((approx_log_f_y_old -approx_log_f_y_new)/approx_log_f_y_old)<tol){
break
}
delta_i = delta_i_new
lambda_i = lambda_i_new
v2_i = v2_i_new
}
return(list(lambda_i=lambda_i, delta_i=delta_i, v2_i=v2_i))
}
## M step: compute the mu and Sigma to maximize the likelihood; closed form
mu_old = mu_0
Sigma_old = Sigma_0
lambda_i_old = matrix(rnorm(p*n), n, p)
v2_i_old = matrix(rnorm(p*n), n, p)^2
delta_i_old = matrix(rnorm(1*n), n, 1)
Sigma_inv_old = solve(Sigma_old)
E_update = lapply(1:n, function(i)
update_cell_i(yi, mu_0=mu_old, Sigma_0=Sigma_old, Sigma_inv = Sigma_inv_old, i, iteration=2000,
lambda_i = lambda_i_old[i,],
v2_i = v2_i_old[i,],
delta_i = delta_i_old[i,]))
lambda_i_new = do.call(rbind, sapply(E_update,`[`, 1))
## E step: optimize the variational parameters for each cell; numerical optimization
# for cell i:
Sigma_inv = solve(Sigma_0)
update_cell_i = function(yi, mu_0, Sigma_0, Sigma_inv, i, iteration=50, lambda_i=NULL, v2_i=NULL, delta_i=NULL){
if(is.null(lambda_i)){lambda_i  = matrix(rnorm(p*1), 1, p)}else{lambda_i=lambda_i+ matrix(rnorm(p*1), 1, p)*max(abs(lambda_i))/10}
#if(is.null(v2_i)){v2_i = (matrix(rnorm(p*1), 1, p)^2)}else{v2_i = v2_i+matrix(runif(n=p*1, min=0, max=max(v2_i)/10), 1, p)}
v2_i = (matrix(rnorm(p*1), 1, p)^2) # seems difficult for numerical optimization
if(is.null(delta_i)){delta_i=1}else{delta_i = delta_i*runif(1, min=0.8, max=1.2)}
tol = 1e-6
mi = sum(yi[i,])
count=0
# update
while(count<iteration){  # do partial update, to save time
print(count)
count=count+1
# write own v2_i update code; can be separate for each entry
# or can actually solve for root! (rootSolve seems not accurate)
one_v2 = function(v2_i_old_j, j, Sigma_inv, mi, delta_i, lambda_i){
-mi/delta_i*exp(lambda_i[j]+1/2*v2_i_old_j)-1/2*v2_i_old_j*Sigma_inv[j,j]+1/2*log(v2_i_old_j)
}
v2_i_new = sapply(1:p, function(j){
optim(v2_i[j], one_v2, 'Brent',j=j, Sigma_inv = Sigma_inv, mi=mi, delta_i=delta_i, lambda_i=lambda_i, control=list('fnscale'=-1, 'maxit'=iteration))$par}
)
delta_i_new = 1+sum(exp(lambda_i+1/2*v2_i_new))
lambda_i_new = optim(lambda_i, approx_log_f_y, 'CG', v2_i=v2_i_new, delta_i=delta_i_new, mu_0=mu_0, Sigma_0=Sigma_0, Sigma_inv=Sigma_inv,i=i,mi=mi, control=list('fnscale'=-1, 'maxit'=iteration) )$par
delta_i_new = 1+sum(exp(lambda_i_new+1/2*v2_i_new))
# if(abs(delta_i-delta_i_new)<tol & mean(abs(lambda_i - lambda_i_new))< tol & mean(abs(v2_i - v2_i_new))<tol){
#   break
# }
approx_log_f_y_old = approx_log_f_y(lambda_i, v2_i, delta_i, mu_0, Sigma_0, Sigma_inv, i, mi)
approx_log_f_y_new = approx_log_f_y(lambda_i_new, v2_i_new, delta_i_new, mu_0, Sigma_0, Sigma_inv, i, mi)
if(abs((approx_log_f_y_old -approx_log_f_y_new)/approx_log_f_y_old)<tol){
break
}
delta_i = delta_i_new
lambda_i = lambda_i_new
v2_i = v2_i_new
}
return(list(lambda_i=lambda_i, delta_i=delta_i, v2_i=v2_i))
}
## M step: compute the mu and Sigma to maximize the likelihood; closed form
mu_old = mu_0
Sigma_old = Sigma_0
lambda_i_old = matrix(rnorm(p*n), n, p)
v2_i_old = matrix(rnorm(p*n), n, p)^2
delta_i_old = matrix(rnorm(1*n), n, 1)
Sigma_inv_old = solve(Sigma_old)
E_update = lapply(1:n, function(i)
update_cell_i(yi, mu_0=mu_old, Sigma_0=Sigma_old, Sigma_inv = Sigma_inv_old, i, iteration=2000,
lambda_i = lambda_i_old[i,],
v2_i = v2_i_old[i,],
delta_i = delta_i_old[i,]))
## E step: optimize the variational parameters for each cell; numerical optimization
# for cell i:
Sigma_inv = solve(Sigma_0)
update_cell_i = function(yi, mu_0, Sigma_0, Sigma_inv, i, iteration=50, lambda_i=NULL, v2_i=NULL, delta_i=NULL){
if(is.null(lambda_i)){lambda_i  = matrix(rnorm(p*1), 1, p)}else{lambda_i=lambda_i+ matrix(rnorm(p*1), 1, p)*max(abs(lambda_i))/10}
#if(is.null(v2_i)){v2_i = (matrix(rnorm(p*1), 1, p)^2)}else{v2_i = v2_i+matrix(runif(n=p*1, min=0, max=max(v2_i)/10), 1, p)}
v2_i = (matrix(rnorm(p*1), 1, p)^2) # seems difficult for numerical optimization
if(is.null(delta_i)){delta_i=1}else{delta_i = delta_i*runif(1, min=0.8, max=1.2)}
tol = 1e-6
mi = sum(yi[i,])
count=0
# update
while(count<iteration){  # do partial update, to save time
print(count)
count=count+1
# write own v2_i update code; can be separate for each entry
# or can actually solve for root! (rootSolve seems not accurate)
one_v2 = function(v2_i_old_j, j, Sigma_inv, mi, delta_i, lambda_i){
-mi/delta_i*exp(lambda_i[j]+1/2*v2_i_old_j)-1/2*v2_i_old_j*Sigma_inv[j,j]+1/2*log(v2_i_old_j)
}
v2_i_new = sapply(1:p, function(j)optim(v2_i[j], one_v2, 'Brent',j=j, Sigma_inv = Sigma_inv, mi=mi, delta_i=delta_i, lambda_i=lambda_i, control=list('fnscale'=-1, 'maxit'=iteration))$par)
delta_i_new = 1+sum(exp(lambda_i+1/2*v2_i_new))
lambda_i_new = optim(lambda_i, approx_log_f_y, 'CG', v2_i=v2_i_new, delta_i=delta_i_new, mu_0=mu_0, Sigma_0=Sigma_0, Sigma_inv=Sigma_inv,i=i,mi=mi, control=list('fnscale'=-1, 'maxit'=iteration) )$par
delta_i_new = 1+sum(exp(lambda_i_new+1/2*v2_i_new))
# if(abs(delta_i-delta_i_new)<tol & mean(abs(lambda_i - lambda_i_new))< tol & mean(abs(v2_i - v2_i_new))<tol){
#   break
# }
approx_log_f_y_old = approx_log_f_y(lambda_i, v2_i, delta_i, mu_0, Sigma_0, Sigma_inv, i, mi)
approx_log_f_y_new = approx_log_f_y(lambda_i_new, v2_i_new, delta_i_new, mu_0, Sigma_0, Sigma_inv, i, mi)
if(abs((approx_log_f_y_old -approx_log_f_y_new)/approx_log_f_y_old)<tol){
break
}
delta_i = delta_i_new
lambda_i = lambda_i_new
v2_i = v2_i_new
}
return(list(lambda_i=lambda_i, delta_i=delta_i, v2_i=v2_i))
}
## M step: compute the mu and Sigma to maximize the likelihood; closed form
mu_old = mu_0
Sigma_old = Sigma_0
lambda_i_old = matrix(rnorm(p*n), n, p)
v2_i_old = matrix(rnorm(p*n), n, p)^2
delta_i_old = matrix(rnorm(1*n), n, 1)
Sigma_inv_old = solve(Sigma_old)
debugSource('~/Desktop/Dropbox/Jing_Ali/variational EM code.R', echo=TRUE)
library(pracma)
set.seed(1)
p=30
n=50
Sigma_de = matrix(rnorm(p*p), p, p)
Sigma = Sigma_de %*% t(Sigma_de)
cond(Sigma)
debugSource('~/Desktop/Dropbox/Jing_Ali/variational EM code.R', echo=TRUE)
mu = rnorm(p)/5+2
exp(c(mu,0))/sum(exp(c(mu,0))) # the true compositions
# generate x based on normal prior of z; these are for n cell samples
zi = t(Sigma_de%*%matrix(rnorm(p*n), nrow=p, ncol=n)+mu)
z0 = rep(0, n)
xi = exp(cbind(zi,z0))/rowSums(exp(cbind(zi,z0)))
m = rpois(n, lambda=1000)
yi = t(sapply(1:n, function(i)rmultinom(1, size=m[i], prob=xi[i,])))
yi
comp_naive = colMeans(yi/rowSums(yi))
yi_naive_comp_pos = (yi+0.5)/rowSums(yi+0.5)
z_naive = log(yi_naive_comp_pos/yi_naive_comp_pos[,p+1])[,-(p+1)]
mu_0 = colMeans(z_naive)
Sigma_0 = cov(z_naive)
E_q_log_f_y_given_z = function(lambda_i, delta_i, v2_i, i, mi){
tmp1=lgamma(mi)
tmp2=- sum(lgamma(yi[i,yi[i,]!=0]))
tmp3= sum(yi[i,-(p+1)]*lambda_i)
tmp4=-mi*(log(delta_i)-1+1/delta_i+1/delta_i*sum(exp(lambda_i+1/2*v2_i)))
tmp5=tmp1+tmp2+tmp3+tmp4
return(tmp5)
}
E_q_log_f_z =function(lambda_i, v2_i, mu_0, Sigma_0, Sigma_inv, i, mi){
-1/2*log(det(Sigma_0))-p/2*log(2*pi)-1/2*(matrix(lambda_i,1,p)-mu_0)%*%Sigma_inv%*%t(matrix(lambda_i,1,p)-mu_0)-1/2*sum(diag(Sigma_inv)*v2_i)
}
H_q = function(v2_i, i){
sum(1/2*log(2*pi*exp(1)*v2_i))
}
approx_log_f_y = function(lambda_i, v2_i, delta_i, mu_0, Sigma_0, Sigma_inv, i, mi){
E_q_log_f_y_given_z(lambda_i, delta_i, v2_i, i, mi) + E_q_log_f_z(matrix(lambda_i,1,p), v2_i, mu_0, Sigma_0, Sigma_inv, i, mi) + H_q(v2_i, i)
}
## E step: optimize the variational parameters for each cell; numerical optimization
# for cell i:
Sigma_inv = solve(Sigma_0)
update_cell_i = function(yi, mu_0, Sigma_0, Sigma_inv, i, iteration=50, lambda_i=NULL, v2_i=NULL, delta_i=NULL){
if(is.null(lambda_i)){lambda_i  = matrix(rnorm(p*1), 1, p)}else{lambda_i=lambda_i+ matrix(rnorm(p*1), 1, p)*max(abs(lambda_i))/10}
#if(is.null(v2_i)){v2_i = (matrix(rnorm(p*1), 1, p)^2)}else{v2_i = v2_i+matrix(runif(n=p*1, min=0, max=max(v2_i)/10), 1, p)}
v2_i = (matrix(rnorm(p*1), 1, p)^2) # seems difficult for numerical optimization
if(is.null(delta_i)){delta_i=1}else{delta_i = delta_i*runif(1, min=0.8, max=1.2)}
tol = 1e-6
mi = sum(yi[i,])
count=0
# update
while(count<iteration){  # do partial update, to save time
print(count)
count=count+1
# write own v2_i update code; can be separate for each entry
# or can actually solve for root! (rootSolve seems not accurate)
one_v2 = function(v2_i_old_j, j, Sigma_inv, mi, delta_i, lambda_i){
-mi/delta_i*exp(lambda_i[j]+1/2*v2_i_old_j)-1/2*v2_i_old_j*Sigma_inv[j,j]+1/2*log(v2_i_old_j)
}
v2_i_new = sapply(1:p, function(j)optim(v2_i[j], one_v2, 'Brent',j=j, Sigma_inv = Sigma_inv, mi=mi, delta_i=delta_i, lambda_i=lambda_i, control=list('fnscale'=-1, 'maxit'=iteration))$par)
delta_i_new = 1+sum(exp(lambda_i+1/2*v2_i_new))
lambda_i_new = optim(lambda_i, approx_log_f_y, 'CG', v2_i=v2_i_new, delta_i=delta_i_new, mu_0=mu_0, Sigma_0=Sigma_0, Sigma_inv=Sigma_inv,i=i,mi=mi, control=list('fnscale'=-1, 'maxit'=iteration) )$par
delta_i_new = 1+sum(exp(lambda_i_new+1/2*v2_i_new))
# if(abs(delta_i-delta_i_new)<tol & mean(abs(lambda_i - lambda_i_new))< tol & mean(abs(v2_i - v2_i_new))<tol){
#   break
# }
approx_log_f_y_old = approx_log_f_y(lambda_i, v2_i, delta_i, mu_0, Sigma_0, Sigma_inv, i, mi)
approx_log_f_y_new = approx_log_f_y(lambda_i_new, v2_i_new, delta_i_new, mu_0, Sigma_0, Sigma_inv, i, mi)
if(abs((approx_log_f_y_old -approx_log_f_y_new)/approx_log_f_y_old)<tol){
break
}
delta_i = delta_i_new
lambda_i = lambda_i_new
v2_i = v2_i_new
}
return(list(lambda_i=lambda_i, delta_i=delta_i, v2_i=v2_i))
}
## M step: compute the mu and Sigma to maximize the likelihood; closed form
mu_old = mu_0
Sigma_old = Sigma_0
lambda_i_old = matrix(rnorm(p*n), n, p)
v2_i_old = matrix(rnorm(p*n), n, p)^2
delta_i_old = matrix(rnorm(1*n), n, 1)
Sigma_inv_old = solve(Sigma_old)
E_update = lapply(1:n, function(i)
update_cell_i(yi, mu_0=mu_old, Sigma_0=Sigma_old, Sigma_inv = Sigma_inv_old, i, iteration=2000,
lambda_i = lambda_i_old[i,],
v2_i = v2_i_old[i,],
delta_i = delta_i_old[i,]))
update_cell_i(yi, mu_0=mu_old, Sigma_0=Sigma_old, Sigma_inv = Sigma_inv_old, i, iteration=20,
lambda_i = lambda_i_old[i,],
v2_i = v2_i_old[i,],
delta_i = delta_i_old[i,])
update_cell_i(yi, mu_0=mu_old, Sigma_0=Sigma_old, Sigma_inv = Sigma_inv_old, i=1, iteration=20,
lambda_i = lambda_i_old[i,],
v2_i = v2_i_old[i,],
delta_i = delta_i_old[i,])
i=1
update_cell_i(yi, mu_0=mu_old, Sigma_0=Sigma_old, Sigma_inv = Sigma_inv_old, i, iteration=20,
lambda_i = lambda_i_old[i,],
v2_i = v2_i_old[i,],
delta_i = delta_i_old[i,])
update_cell_i(yi, mu_0=mu_old, Sigma_0=Sigma_old, Sigma_inv = Sigma_inv_old, i, iteration=200,
lambda_i = lambda_i_old[i,],
v2_i = v2_i_old[i,],
delta_i = delta_i_old[i,])
i=1
update_cell_i(yi, mu_0=mu_old, Sigma_0=Sigma_old, Sigma_inv = Sigma_inv_old, i, iteration=2000,
lambda_i = lambda_i_old[i,],
v2_i = v2_i_old[i,],
delta_i = delta_i_old[i,])
## E step: optimize the variational parameters for each cell; numerical optimization
# for cell i:
Sigma_inv = solve(Sigma_0)
update_cell_i = function(yi, mu_0, Sigma_0, Sigma_inv, i, iteration=50, lambda_i=NULL, v2_i=NULL, delta_i=NULL){
if(is.null(lambda_i)){lambda_i  = matrix(rnorm(p*1), 1, p)}else{lambda_i=lambda_i+ matrix(rnorm(p*1), 1, p)*max(abs(lambda_i))/10}
#if(is.null(v2_i)){v2_i = (matrix(rnorm(p*1), 1, p)^2)}else{v2_i = v2_i+matrix(runif(n=p*1, min=0, max=max(v2_i)/10), 1, p)}
v2_i = (matrix(rnorm(p*1), 1, p)^2) # seems difficult for numerical optimization
if(is.null(delta_i)){delta_i=1}else{delta_i = delta_i*runif(1, min=0.8, max=1.2)}
tol = 1e-6
mi = sum(yi[i,])
count=0
# update
while(count<iteration){  # do partial update, to save time
print(count)
count=count+1
# write own v2_i update code; can be separate for each entry
# or can actually solve for root! (rootSolve seems not accurate)
one_v2 = function(v2_i_old_j, j, Sigma_inv, mi, delta_i, lambda_i){
-mi/delta_i*exp(lambda_i[j]+1/2*v2_i_old_j)-1/2*v2_i_old_j*Sigma_inv[j,j]+1/2*log(v2_i_old_j)
}
v2_i_new = sapply(1:p, function(j)optim(v2_i[j], one_v2, 'Brent',j=j, Sigma_inv = Sigma_inv, mi=mi, delta_i=delta_i, lambda_i=lambda_i, control=list('fnscale'=-1))$par)
delta_i_new = 1+sum(exp(lambda_i+1/2*v2_i_new))
lambda_i_new = optim(lambda_i, approx_log_f_y, 'CG', v2_i=v2_i_new, delta_i=delta_i_new, mu_0=mu_0, Sigma_0=Sigma_0, Sigma_inv=Sigma_inv,i=i,mi=mi, control=list('fnscale'=-1) )$par
delta_i_new = 1+sum(exp(lambda_i_new+1/2*v2_i_new))
# if(abs(delta_i-delta_i_new)<tol & mean(abs(lambda_i - lambda_i_new))< tol & mean(abs(v2_i - v2_i_new))<tol){
#   break
# }
approx_log_f_y_old = approx_log_f_y(lambda_i, v2_i, delta_i, mu_0, Sigma_0, Sigma_inv, i, mi)
approx_log_f_y_new = approx_log_f_y(lambda_i_new, v2_i_new, delta_i_new, mu_0, Sigma_0, Sigma_inv, i, mi)
if(abs((approx_log_f_y_old -approx_log_f_y_new)/approx_log_f_y_old)<tol){
break
}
delta_i = delta_i_new
lambda_i = lambda_i_new
v2_i = v2_i_new
}
return(list(lambda_i=lambda_i, delta_i=delta_i, v2_i=v2_i))
}
## M step: compute the mu and Sigma to maximize the likelihood; closed form
mu_old = mu_0
Sigma_old = Sigma_0
lambda_i_old = matrix(rnorm(p*n), n, p)
v2_i_old = matrix(rnorm(p*n), n, p)^2
delta_i_old = matrix(rnorm(1*n), n, 1)
Sigma_inv_old = solve(Sigma_old)
i=1
update_cell_i(yi, mu_0=mu_old, Sigma_0=Sigma_old, Sigma_inv = Sigma_inv_old, i, iteration=2000,
lambda_i = lambda_i_old[i,],
v2_i = v2_i_old[i,],
delta_i = delta_i_old[i,])
library(pracma)
set.seed(1)
p=30
n=50
Sigma_de = matrix(rnorm(p*p), p, p)
Sigma = Sigma_de %*% t(Sigma_de)
cond(Sigma)
mu = rnorm(p)/5+2
exp(c(mu,0))/sum(exp(c(mu,0))) # the true compositions
# generate x based on normal prior of z; these are for n cell samples
zi = t(Sigma_de%*%matrix(rnorm(p*n), nrow=p, ncol=n)+mu)
z0 = rep(0, n)
xi = exp(cbind(zi,z0))/rowSums(exp(cbind(zi,z0)))
m = rpois(n, lambda=1000)
yi = t(sapply(1:n, function(i)rmultinom(1, size=m[i], prob=xi[i,])))
yi
comp_naive = colMeans(yi/rowSums(yi))
yi_naive_comp_pos = (yi+0.5)/rowSums(yi+0.5)
z_naive = log(yi_naive_comp_pos/yi_naive_comp_pos[,p+1])[,-(p+1)]
mu_0 = colMeans(z_naive)
Sigma_0 = cov(z_naive)
E_q_log_f_y_given_z = function(lambda_i, delta_i, v2_i, i, mi){
tmp1=lgamma(mi)
tmp2=- sum(lgamma(yi[i,yi[i,]!=0]))
tmp3= sum(yi[i,-(p+1)]*lambda_i)
tmp4=-mi*(log(delta_i)-1+1/delta_i+1/delta_i*sum(exp(lambda_i+1/2*v2_i)))
tmp5=tmp1+tmp2+tmp3+tmp4
return(tmp5)
}
E_q_log_f_z =function(lambda_i, v2_i, mu_0, Sigma_0, Sigma_inv, i, mi){
-1/2*log(det(Sigma_0))-p/2*log(2*pi)-1/2*(matrix(lambda_i,1,p)-mu_0)%*%Sigma_inv%*%t(matrix(lambda_i,1,p)-mu_0)-1/2*sum(diag(Sigma_inv)*v2_i)
}
H_q = function(v2_i, i){
sum(1/2*log(2*pi*exp(1)*v2_i))
}
approx_log_f_y = function(lambda_i, v2_i, delta_i, mu_0, Sigma_0, Sigma_inv, i, mi){
E_q_log_f_y_given_z(lambda_i, delta_i, v2_i, i, mi) + E_q_log_f_z(matrix(lambda_i,1,p), v2_i, mu_0, Sigma_0, Sigma_inv, i, mi) + H_q(v2_i, i)
}
## E step: optimize the variational parameters for each cell; numerical optimization
# for cell i:
Sigma_inv = solve(Sigma_0)
update_cell_i = function(yi, mu_0, Sigma_0, Sigma_inv, i, iteration=50, lambda_i=NULL, v2_i=NULL, delta_i=NULL){
if(is.null(lambda_i)){lambda_i  = matrix(rnorm(p*1), 1, p)}else{lambda_i=lambda_i+ matrix(rnorm(p*1), 1, p)*max(abs(lambda_i))/10}
#if(is.null(v2_i)){v2_i = (matrix(rnorm(p*1), 1, p)^2)}else{v2_i = v2_i+matrix(runif(n=p*1, min=0, max=max(v2_i)/10), 1, p)}
v2_i = (matrix(rnorm(p*1), 1, p)^2) # seems difficult for numerical optimization
if(is.null(delta_i)){delta_i=1}else{delta_i = delta_i*runif(1, min=0.8, max=1.2)}
tol = 1e-6
mi = sum(yi[i,])
count=0
# update
while(count<iteration){  # do partial update, to save time
print(count)
count=count+1
# write own v2_i update code; can be separate for each entry
# or can actually solve for root! (rootSolve seems not accurate)
one_v2 = function(v2_i_old_j, j, Sigma_inv, mi, delta_i, lambda_i){
-mi/delta_i*exp(lambda_i[j]+1/2*v2_i_old_j)-1/2*v2_i_old_j*Sigma_inv[j,j]+1/2*log(v2_i_old_j)
}
v2_i_new = sapply(1:p, function(j)optim(v2_i[j], one_v2, 'Brent',j=j, Sigma_inv = Sigma_inv, mi=mi, delta_i=delta_i, lambda_i=lambda_i, control=list('fnscale'=-1))$par)
delta_i_new = 1+sum(exp(lambda_i+1/2*v2_i_new))
lambda_i_new = optim(lambda_i, approx_log_f_y, 'CG', v2_i=v2_i_new, delta_i=delta_i_new, mu_0=mu_0, Sigma_0=Sigma_0, Sigma_inv=Sigma_inv,i=i,mi=mi, control=list('fnscale'=-1) )$par
delta_i_new = 1+sum(exp(lambda_i_new+1/2*v2_i_new))
# if(abs(delta_i-delta_i_new)<tol & mean(abs(lambda_i - lambda_i_new))< tol & mean(abs(v2_i - v2_i_new))<tol){
#   break
# }
approx_log_f_y_old = approx_log_f_y(lambda_i, v2_i, delta_i, mu_0, Sigma_0, Sigma_inv, i, mi)
approx_log_f_y_new = approx_log_f_y(lambda_i_new, v2_i_new, delta_i_new, mu_0, Sigma_0, Sigma_inv, i, mi)
if(abs((approx_log_f_y_old -approx_log_f_y_new)/approx_log_f_y_old)<tol){
break
}
delta_i = delta_i_new
lambda_i = lambda_i_new
v2_i = v2_i_new
}
return(list(lambda_i=lambda_i, delta_i=delta_i, v2_i=v2_i))
}
# approx_full_log_f_y = sum(approx_log_f_y)
## M step: compute the mu and Sigma to maximize the likelihood; closed form
mu_old = mu_0
Sigma_old = Sigma_0
lambda_i_old = matrix(rnorm(p*n), n, p)
v2_i_old = matrix(rnorm(p*n), n, p)^2
delta_i_old = matrix(rnorm(1*n), n, 1)
Sigma_inv_old = solve(Sigma_old)
i=1
update_cell_i(yi, mu_0=mu_old, Sigma_0=Sigma_old, Sigma_inv = Sigma_inv_old, i, iteration=2000,
lambda_i = lambda_i_old[i,],
v2_i = v2_i_old[i,],
delta_i = delta_i_old[i,])
E_update = lapply(1:n, function(i)
update_cell_i(yi, mu_0=mu_old, Sigma_0=Sigma_old, Sigma_inv = Sigma_inv_old, i, iteration=2000,
lambda_i = lambda_i_old[i,],
v2_i = v2_i_old[i,],
delta_i = delta_i_old[i,]))
library(pracma)
set.seed(1)
p=30
n=50
Sigma_de = matrix(rnorm(p*p), p, p)
Sigma = Sigma_de %*% t(Sigma_de)
cond(Sigma)
mu = rnorm(p)/5+2
exp(c(mu,0))/sum(exp(c(mu,0))) # the true compositions
###-----------------------------------------------------------
###
### Implementation of available methods for computing microbiome network
###
###-----------------------------------------------------------
filepath = '/Users/Kun/Desktop/Dropbox/Microbial_Networks/microGraph/Kun_code'
#######################
##
##
##  Data Examples for book chapter
##
##
########################
filepath = '/Users/Kun/Desktop/Dropbox/Microbial_Networks/microGraph' # MAC
setwd(filepath)
source('lib/func_libs.R')
apply(matrix(1:6, nrow=2, ncol=3), 1, sum)
target = 'covariance'
grepl(target,'covariance',ignore.case=TRUE)
target = 'CovarianCe'
grepl(target,'covariance',ignore.case=TRUE)
method = 'Conet'
grepl(method,c('CoNet', 'SparCC', 'CCLasso', 'COAT'),ignore.case=TRUE)
library(huge)
