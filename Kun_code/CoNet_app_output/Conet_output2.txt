; export date: Mon Apr 06 22:29:14 PDT 2020
; graph id: 
; comments: # ===># NaN treatment
# Date=Mon Apr 06 22:29:07 PDT 2020
# PARAMETER
# NaN treatment strategy=none
# NaN treatment parameter (minimum number of NaN-free value pairs)=1
# ===># Matrix preprocessor
# Date=Mon Apr 06 22:29:07 PDT 2020
# INPUT
# Rows in input matrix=15
# Columns in input matrix=100
# RESULT
# Rows in processed matrix=15
# Columns in processed matrix=100
# PARAMETER
# Input matrix is an incidence matrix=false
# Matrix processed=true
# Binary feature matrix location=
# Given features are incidence features=false
# Preprocessing steps=[smoother, filter, normalizer, higherleveltaxa, shifter, logtransformer, binconverter]
# Matrix filtering settings=# Matrix filter
# Date=Mon Apr 06 22:29:07 PDT 2020
# INPUT
# Rows in input matrix=15
# Columns in input matrix=100
# RESULT
# Rows in filtered matrix=15
# Columns in filtered matrix=100
# Number of removed rows=0
# Number of removed columns=0
# Removed columns=[]
# PARAMETER
# Incidence matrix=false
# Filter method=row_minocc
# Parameter of filter method=5.0
# Features are omitted from column minimum sum=true
# Features are omitted from row top percent=true
# Column-wise sums of filtered non-feature rows were added as additional row (named summed-nonfeat-rows)=true
# User-provided names of rows to discard=[]
# User-provided names of columns to discard=[]
# row_minocc=Discard rows below given minimum occurrence of values equal to or above 1.0
# row_toppercent=Discard rows whose row sum is below the given percentage of all row sums
# row_minsum=Discard rows whose sum is below the given value
# row_withzero=Discard rows whose number of zeros is below the given value
# col_minocc=Discard columns below given minimum occurrence of values equal to or above 1.0
# col_minsum=Discard columns whose sum is below the given value
# Matrix normalization settings=# Abundance matrix normalization
# Date=Mon Apr 06 22:29:07 PDT 2020
# INPUT
# Rows in input matrix=15
# Columns in input matrix=100
# PARAMETER
# Group attribute=
# Standardization methods applied=col_norm
# Features excluded from standardization=true
# Problematic rows/columns removed=true
# higherleveltaxa was not carried out
# smoother was not carried out
# logtransformer was not carried out
# binconverter was not carried out
# ===># Cooccurrence network construction
# Date=Mon Apr 06 22:29:07 PDT 2020
# INPUT
# Rows in input matrix=15
# Columns in input matrix=100
# RESULT
# Nodes in cooccurrence network=15
# Arcs in cooccurrence network=124
# PARAMETER
# Copresence only computed=false
# Mutual exclusion only computed=false
# Upper threshold=NaN
# Lower threshold=NaN
# Cooccurrence method=ensemble
# Cooccurrence network directed=true
# Network builder methods applied=[correl_pearson, dist_kullbackleibler, correl_spearman, dist_bray]
# Merge strategy=union
# Parameter values of network builder methods=# Data: thresholds 
# identifier: correctionfactorbrown  annotation: correctionfactorbrown  value: 1.4083014975066401 
# identifier: correl_pearson  annotation: upperThreshold  value: 0.08996837370736493  annotation: lowerThreshold  value: -0.22014562593643228 
# identifier: correl_spearman  annotation: upperThreshold  value: 0.08298829882988307  annotation: lowerThreshold  value: -0.1685448544854487 
# identifier: degreesoffreedombrown  annotation: degreesoffreedombrown  value: 5.680601784606339 
# identifier: dist_bray  annotation: upperThreshold  value: 0.3120901052920899  annotation: lowerThreshold  value: 0.2576472510815614 
# identifier: dist_kullbackleibler  annotation: upperThreshold  value: 0.7285041533307338  annotation: lowerThreshold  value: 0.34859800855458223 
# Minimal support required for an edge (in context of strategy minsupport)=2
# Methods excluded from edge filtering (in strategies minsupport, intersection and majority)=[]
# Network built with multi-edges=true
# Preprocessing=# Matrix preprocessor
# Date=Mon Apr 06 22:29:07 PDT 2020
# INPUT
# Rows in input matrix=15
# Columns in input matrix=100
# RESULT
# Rows in processed matrix=15
# Columns in processed matrix=100
# PARAMETER
# Input matrix is an incidence matrix=false
# Matrix processed=true
# Binary feature matrix location=
# Given features are incidence features=false
# Preprocessing steps=[smoother, filter, normalizer, higherleveltaxa, shifter, logtransformer, binconverter]
# Matrix filtering settings=# Matrix filter
# Date=Mon Apr 06 22:29:07 PDT 2020
# INPUT
# Rows in input matrix=15
# Columns in input matrix=100
# RESULT
# Rows in filtered matrix=15
# Columns in filtered matrix=100
# Number of removed rows=0
# Number of removed columns=0
# Removed columns=[]
# PARAMETER
# Incidence matrix=false
# Filter method=row_minocc
# Parameter of filter method=5.0
# Features are omitted from column minimum sum=true
# Features are omitted from row top percent=true
# Column-wise sums of filtered non-feature rows were added as additional row (named summed-nonfeat-rows)=true
# User-provided names of rows to discard=[]
# User-provided names of columns to discard=[]
# row_minocc=Discard rows below given minimum occurrence of values equal to or above 1.0
# row_toppercent=Discard rows whose row sum is below the given percentage of all row sums
# row_minsum=Discard rows whose sum is below the given value
# row_withzero=Discard rows whose number of zeros is below the given value
# col_minocc=Discard columns below given minimum occurrence of values equal to or above 1.0
# col_minsum=Discard columns whose sum is below the given value
# Matrix normalization settings=# Abundance matrix normalization
# Date=Mon Apr 06 22:29:07 PDT 2020
# INPUT
# Rows in input matrix=15
# Columns in input matrix=100
# PARAMETER
# Group attribute=
# Standardization methods applied=col_norm
# Features excluded from standardization=true
# Problematic rows/columns removed=true
# higherleveltaxa was not carried out
# smoother was not carried out
# logtransformer was not carried out
# binconverter was not carried out
# STATISTICS
# Maximal number of possible interactions (total arc number)=420
# Number of exclusive row pairs (subtracted from total arc number in Bonferroni and E-value correction)=0
# Removed arcs=296
# Node number in ensemble cooccurrence network=15
# Edge number in ensemble cooccurrence network=124
# Number of node pairs in ensemble cooccurrence network=61
# Edge number contributed by method correl_pearson=31 
# Edge percentage contributed by method correl_pearson=25.0 
# Edge number contributed by method dist_kullbackleibler=31 
# Edge percentage contributed by method dist_kullbackleibler=25.0 
# Edge number contributed by method correl_spearman=31 
# Edge percentage contributed by method correl_spearman=25.0 
# Edge number contributed by method dist_bray=31 
# Edge percentage contributed by method dist_bray=25.0 
# Number of node pairs supported by 1 methods=19
# Number of node pairs supported by 2 methods=26
# Number of node pairs supported by 3 methods=11
# Number of node pairs supported by 4 methods=5
# Minimal number of methods (support) for a node pair=1.0
# Maximal number of methods (support) for a node pair=4.0
# Mean number of methods (support) for a node pair=2.0327868852459017
# Median number of methods (support) for a node pair=2.0
# NETWORK SCORE: 2.0327868852459017
# ===># Cooccurrence network construction score computation using random matrices
# Date=Mon Apr 06 22:29:13 PDT 2020
# INPUT
# Rows in input matrix=15
# Columns in input matrix=100
# MATRIX NORMALIZATION
# Standardization methods=col_norm
# COOCCURRENCE CALCULATION
# Co-occurrence method=ensemble
# Lower threshold of co-occurrence method=NaN
# Upper threshold of co-occurrence method=NaN
# PARAMETER
# Resampling method=Bootstrap
# Number of resampled columns=100
# Probability distribution used for column index choice=uniform
# Randomization routine=edgeScores
# Allowed number of false discoveries (for randomization routine lallich only)=0
# Iteration number=100
# Scoring method=avg-edgenum-by-nodepair
# Matrix randomized from previous randomized matrix=false
# Re-normalization enabled=false
# ===># P-value from score vector computation
# Date=Mon Apr 06 22:29:13 PDT 2020
# INPUT
# Number of random scores (without original score)=100
# Mean of random scores (without original score)=4.0
# Median of random scores (without original score)=4.0
# Maximum of random scores (without original score)=4.0
# Minimum of random scores (without original score)=4.0
# Standard deviation of random scores (without original score)=0.0
# Original score=2.0327868852459017
# PARAMETER
# NaN omitted=false
# P-value computation method=distribFree
# Tail of score distribution used for p-value computation=right
# Null value (for two-sided test only)=0.0
# RESULT
# P-value=1.0
# Error margin (can only be computed for one-sided test)=0.0
# NETWORK P-VALUE: 1.0
# RANDOMIZATION EDGE FILTER 
# ===># Cooccurrence network construction
# Date=Mon Apr 06 22:29:14 PDT 2020
# INPUT
# Rows in input matrix=15
# Columns in input matrix=100
# RESULT
# Nodes in cooccurrence network=68
# Arcs in cooccurrence network=124
# Number of arcs removed by randomization filter=0
# PARAMETER
# Copresence only computed=false
# Mutual exclusion only computed=false
# Upper threshold=1.0
# Lower threshold=1.0
# Cooccurrence method=edgescores
# Cooccurrence network directed=true
# Multiple test correction (for return types E-value and significance)=none
# Return type=pval
# Lower threshold 1.0 interpreted as threshold on p-value, E-value or significance.
# Upper threshold 1.0 interpreted as threshold on p-value, E-value or significance.
# Score vectors contain original score=true
# Bottom scores also tested for signficance (and smallest p-value kept)=false
# Edge weights represent merged scores=false
# Distribution rinsed (i.e. multiple occurrences of the same value removed, does not apply to Lallich)=false
# P-value merge strategy (method-specific cut-offs are not computed if a merge strategy was set)=brown
# Ensemble method: Scores merged (no multi- or separate graph, has impact on distribution tail selected for p-value computation)=false
# Number of tests carried out (relevant for Bonferroni and E-value correction):=420
# Number of excluded row pairs (relevant for Bonferroni and E-value correction):=0
# Minimal number of NaN-free scores required per edge=10
# Arcs not present in enough random networks are added to the output network=false
# Number of arcs that do not appear in enough random networks=0
# NaNs in distribution taken into account in (distribution-free) p-value computation (applied only if edge is not filtered out because of too few non-NaN values)=false
# Bootstrap p-values computed by comparison of bootstrap distribution to null distribution=true
# Null distributions given in file=E:\Dropbox\Microbial_Networks\codes\data\Conet_step1_output.txt
# Test used to compare bootstrap and permutation (null) distributions=pnorm
# PNorm test computed in R=false
# Gaussians fitted from sample mean and standard deviation (instead of normFit)=true
# Measures for which null values were computed with the null value provider or from the all-pairwise distribution file rather than taken from the given null distributions=[]
# Metric used to compute original network (if applicable)=
# Original threshold set=NaN
# Unknown random network edge scores treated as zero=false
# Number of re-samplings=100
# Cooccurrence method used to generate edge scores=ensemble
# Randomization routine used to compute scores=edgeScores
# Number of arcs in the original network=124
# CONFIDENCE INTERVAL EDGE FILTER 
# # Network filter
# Date=Mon Apr 06 22:29:14 PDT 2020
# INPUT
# Nodes in input network=15
# Arcs in input network=124
# PARAMETER
# Attributes of reverse edges considered if direct edges did not provide the attribute value=false
# Shift of original score with respect to score distribution computed as p-value. Edge removed if p-value below=0.025
# Shift of original score with respect to score distribution computed as p-value. Edge removed if p-value above=0.975
# RESULT
# Number of filtered edges=0
# Nodes in filtered network=15
# Arcs in filtered network=124

# P-VALUE EDGE FILTER 
# ===># Network filter
# Date=Mon Apr 06 22:29:14 PDT 2020
# INPUT
# Nodes in input network=15
# Arcs in input network=56
# PARAMETER
# Multiple test correction method=benjaminihochberg
# Return type=pval
# Number of tests (for Bonferroni and E-value correction only)=358.0
# Threshold=0.05
# RESULT
# Number of filtered edges=55
# Nodes in filtered network=2
# Arcs in filtered network=1
# Number of edges removed because of conflicting interaction types: 20
# ===># Implementation of measures
# Date=Mon Apr 06 22:29:14 PDT 2020
# Spearman correlation=Jean-Sebastien Lerat's library
# Pearson correlation=Jean-Sebastien Lerat's library
# Kullback Leibler dissimilarity=Jean-Sebastien Lerat's library
# Bray Curtis distance=Jean-Sebastien Lerat's library
# RUNTIME: 6736 ms 
# VERSION: 1.1.1.beta
# CALL: # Note that you need to point to the CoNet jar file using -cp and that a number of options have to be set via a configuration file. You can open the command line help page to learn more about CoNet on command line. The following options have to be set via a configuration file (check the help or command line option -Z for configuration file parameter names): LineageSeparator, MiImplementation, Poolvar, DisableSpeedup, NoRserveDependency, PseudoCounts, RserveHost, RservePort. If you export a random score file, please make sure no previous random score file exists at the same location.
java be.ac.vub.bsb.cooccurrence.cmd.CooccurrenceAnalyser --method ensemble --input E:\Dropbox\Microbial_Networks\codes\data\data.txt --output E:\Dropbox\Microbial_Networks\codes\data\Conet_output2.txt --correlnonrandp none --multicorr benjaminihochberg --thresholdguessing edgeNumber --randroutine edgeScores --randscorefile E:\Dropbox\Microbial_Networks\codes\data\Conet_step2_output.txt --ensemblemethods correl_pearson/correl_spearman/dist_bray/dist_kullbackleibler --nulldistribfile E:\Dropbox\Microbial_Networks\codes\data\Conet_step1_output.txt --max 3 --kernelwidth 0.25 --minetdisc equalfreq --minetmiestimator mi.shrink --networkmergestrategy union --nantreatment none --nantreatmentparam 1 --multigraph --scoremergestrategy mean --minsupport 2 --min 1 --measure1 conf --measure2 supp --scoreexport --topbottom --keepfilteredrows --matrixtype count --stand col_norm --guessingparam 15.0 --resamplemethod bootstrap --verbosity FATAL --iterations 100 --edgethreshold 0.05 --inference mrnet --pvaluemerge brown --filter row_minocc/confidence_boot/rand --filterparameter 5.0 --output E:\Dropbox\Microbial_Networks\codes\data\Conet_output2.txt

;NODES	Label	oriID	degree	posdegree	negdegree	unknowndegree	abundance	samplecount
taxa3	taxa3	taxa3	1	0	1	0	23.4092501828308	100
taxa1	taxa1	taxa1	1	0	1	0	23.354949422283507	100
;ARCS	interactionType	weight	cooc_method	method_number	randdistribMean	randdistribMedian	randdistribSD	randdistribNumNonNaNScores	pval	qval	pval-brown-merge	nulldistribsd	nulldistribmean	nulldistribmedian	oriScore	methodname_score	methodname_interactiontype	methodname_pval
taxa1	taxa3	[mutualExclusion, mutualExclusion, mutualExclusion, mutualExclusion]	0.0	[dist_bray, dist_kullbackleibler, correl_pearson, correl_spearman]	4	[0.40216294232294686, 1.0219798700156837, -0.9931749026003268, -0.9910429231006329]	[0.4028394145097353, 1.0311131904305069, -0.9932352699629794, -0.991476104249277]	[0.02426480032014002, 0.10607434719323411, 0.0010975040743978475, 0.001928051184722064]	[100.0, 100.0, 100.0, 100.0]	0.0	0.0	0.0	0.01634972055115202	0.27258875350000006	0.273380805	0.40544416345996004	[dist_bray=0.40544416345996004, dist_kullbackleibler=1.0405734904772892, correl_pearson=-0.9933574010358345, correl_spearman=-0.9928472847284737]	[dist_bray=mutualExclusion, dist_kullbackleibler=mutualExclusion, correl_pearson=mutualExclusion, correl_spearman=mutualExclusion]	[dist_bray=0.20275485507026814, dist_kullbackleibler=0.0641007759278529, correl_pearson=0.0, correl_spearman=0.0]
